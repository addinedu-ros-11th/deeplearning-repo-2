{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8163895",
   "metadata": {},
   "source": [
    "# CCTV ë‚™ìƒ/ëˆ„ì›€ ê°ì§€ ë°ì´í„°ì…‹ ë”¥ëŸ¬ë‹ í•™ìŠµ ì‹¤ìŠµ\n",
    "ì´ ë…¸íŠ¸ë¶ì€ CCTV Incident Dataset (Fall and Lying Down Detection)ì„ í™œìš©í•˜ì—¬ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ê°ì²´ ê²€ì¶œ ë° ë¶„ë¥˜ ëª¨ë¸ì„ í•™ìŠµ, í‰ê°€, ì €ì¥í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006af01f",
   "metadata": {},
   "source": [
    "## 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "ë”¥ëŸ¬ë‹ í•™ìŠµ ë° ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02bde00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b96771",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "YOLO í•™ìŠµì„ ìœ„í•´ ë°ì´í„°ì…‹ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ê³ , í•„ìš”í•œ ê²½ìš° data.yaml íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "- ë°ì´í„° ê²½ë¡œ: /home/clyde/dev_ws/deeplearning-repo-2/data/CCTV_Incident_Fall_Lying_Dataset/laying_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92051250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„°ì…‹ í´ë” êµ¬ì¡°:\n",
      "/home/clyde/dev_ws/deeplearning-repo-2/data/CCTV_Incident_Fall_Lying_Dataset/laying_dataset 3 files\n"
     ]
    }
   ],
   "source": [
    "# YOLO í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ ê²½ë¡œ ì§€ì • ë° êµ¬ì¡° í™•ì¸\n",
    "base_dir = '/home/clyde/dev_ws/deeplearning-repo-2/data/CCTV_Incident_Fall_Lying_Dataset/laying_dataset'\n",
    "print('ë°ì´í„°ì…‹ í´ë” êµ¬ì¡°:')\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    print(root, len(files), 'files')\n",
    "    # í•˜ìœ„ í´ë”ê¹Œì§€ ëª¨ë‘ ì¶œë ¥í•˜ë ¤ë©´ break ì œê±°\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091cff4",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° train/val ìë™ ë¶„ë¦¬\n",
    "- ì›ë³¸ laying_dataset í´ë”ì˜ .png/.txt íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ, ì •ìƒ ë ˆì´ë¸”ë§Œ ê³¨ë¼ 8:2ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
    "- images/train, images/val, labels/train, labels/val í´ë”ê°€ ìë™ ìƒì„±ë©ë‹ˆë‹¤.\n",
    "- ë ˆì´ë¸” í˜•ì‹ ì˜¤ë¥˜ê°€ ìˆëŠ” íŒŒì¼ì€ ë³µì‚¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03889f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 88ê°œ, val: 23ê°œ (ì •ìƒ ë ˆì´ë¸”ë§Œ ë³µì‚¬)\n"
     ]
    }
   ],
   "source": [
    "# [Step 1] ì›ë³¸ì—ì„œ train/val ìë™ ë¶„ë¦¬ (ì˜¤ë¥˜ ì—†ëŠ” ë ˆì´ë¸”ë§Œ ì‚¬ìš©)\n",
    "# - ì›ë³¸ laying_dataset í´ë”ì˜ .png/.txt íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ, ì •ìƒ ë ˆì´ë¸”ë§Œ ê³¨ë¼ 8:2ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
    "# - images/train, images/val, labels/train, labels/val í´ë”ê°€ ìë™ ìƒì„±ë©ë‹ˆë‹¤.\n",
    "# - ë ˆì´ë¸” í˜•ì‹ ì˜¤ë¥˜ê°€ ìˆëŠ” íŒŒì¼ì€ ë³µì‚¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "import shutil, random\n",
    "\n",
    "# ì›ë³¸ ì´ë¯¸ì§€/ë ˆì´ë¸” ê²½ë¡œ\n",
    "img_dir = os.path.join(base_dir, 'images')\n",
    "lbl_dir = os.path.join(base_dir, 'labels')\n",
    "\n",
    "# .png íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬\n",
    "all_imgs = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "all_imgs.sort()\n",
    "random.seed(42)\n",
    "random.shuffle(all_imgs)\n",
    "\n",
    "# í´ë” ìƒì„± í•¨ìˆ˜\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def is_valid_label(lbl_path):\n",
    "    try:\n",
    "        with open(lbl_path) as fp:\n",
    "            for line in fp:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) < 5:\n",
    "                    return False\n",
    "                try:\n",
    "                    cls_id = int(float(parts[0]))\n",
    "                    nums = [float(x) for x in parts[1:]]\n",
    "                except ValueError:\n",
    "                    return False\n",
    "                if cls_id < 0:\n",
    "                    return False\n",
    "                # bbox: x, y, w, h in [0,1]\n",
    "                bbox = nums[:4]\n",
    "                if any((x < 0.0 or x > 1.0) for x in bbox):\n",
    "                    return False\n",
    "                # keypoints (if present): x, y in [0,1], v in [0,2]\n",
    "                if len(nums) > 4:\n",
    "                    kpts = nums[4:]\n",
    "                    if len(kpts) % 3 != 0:\n",
    "                        return False\n",
    "                    for i in range(0, len(kpts), 3):\n",
    "                        x, y, v = kpts[i:i+3]\n",
    "                        if x < 0.0 or x > 1.0:\n",
    "                            return False\n",
    "                        if y < 0.0 or y > 1.0:\n",
    "                            return False\n",
    "                        if v < 0.0 or v > 2.0:\n",
    "                            return False\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "\n",
    "valid_imgs = []\n",
    "invalid_imgs = []\n",
    "for img in all_imgs:\n",
    "    lbl_path = os.path.join(lbl_dir, img.replace('.png', '.txt'))\n",
    "    if is_valid_label(lbl_path):\n",
    "        valid_imgs.append(img)\n",
    "    else:\n",
    "        invalid_imgs.append(img)\n",
    "\n",
    "n_total = len(valid_imgs)\n",
    "n_train = int(n_total * 0.8)\n",
    "train_imgs = valid_imgs[:n_train]\n",
    "val_imgs = valid_imgs[n_train:]\n",
    "\n",
    "# train/val í´ë”ì— ì´ë¯¸ì§€ì™€ ì •ìƒ ë ˆì´ë¸”ë§Œ ë³µì‚¬\n",
    "for split, img_list in zip(['train', 'val'], [train_imgs, val_imgs]):\n",
    "    img_out = os.path.join(base_dir, 'images', split)\n",
    "    lbl_out = os.path.join(base_dir, 'labels', split)\n",
    "    ensure_dir(img_out)\n",
    "    ensure_dir(lbl_out)\n",
    "    for img in img_list:\n",
    "        lbl = img.replace('.png', '.txt')\n",
    "        shutil.copy2(os.path.join(img_dir, img), img_out)\n",
    "        shutil.copy2(os.path.join(lbl_dir, lbl), lbl_out)\n",
    "print(f\"train: {len(train_imgs)}ê°œ, val: {len(val_imgs)}ê°œ (ì •ìƒ ë ˆì´ë¸”ë§Œ ë³µì‚¬)\")\n",
    "if invalid_imgs:\n",
    "    print(f\"ìŠ¤í‚µ: {len(invalid_imgs)}ê°œ (ë ˆì´ë¸” ì˜¤ë¥˜/ëˆ„ë½)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae2a9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ìœ„ ì…€ì„ ì‹¤í–‰í•˜ë©´, ì •ìƒ ë ˆì´ë¸”ë§Œ í¬í•¨ëœ train/val ë°ì´í„°ì…‹ì´ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤.\n",
    "ì´ì œ ì•„ë˜ ë‹¨ê³„ë¡œ data.yaml íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1db409",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "- data.yaml íŒŒì¼ì„ ìƒì„±í•˜ì—¬ í•™ìŠµì— í•„ìš”í•œ ê²½ë¡œ, í´ë˜ìŠ¤ ìˆ˜, í´ë˜ìŠ¤ ì´ë¦„ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b96f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.yaml íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# data.yaml íŒŒì¼ ìë™ ìƒì„± ì˜ˆì‹œ (pose)\n",
    "\n",
    "import yaml\n",
    "\n",
    "data_yaml = {\n",
    "    'train': f'{base_dir}/images/train',\n",
    "    'val': f'{base_dir}/images/val',\n",
    "    'kpt_shape': [17, 3],  # keypoints: 17ê°œ, (x, y, v)\n",
    "    'nc': 2,  # í´ë˜ìŠ¤ ìˆ˜ (ì˜ˆì‹œ: ë‚™ìƒ, ëˆ„ì›€)\n",
    "    'names': ['fall', 'lying']\n",
    "}\n",
    "\n",
    "with open(f'{base_dir}/data.yaml', 'w') as f:\n",
    "    yaml.dump(data_yaml, f, allow_unicode=True)\n",
    "print('data.yaml íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a375570",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ì„¤ê³„ ë° í•™ìŠµ\n",
    "- Ultralytics YOLOv8 ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ, ìƒì„±í•œ data.yamlì„ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e767e767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache ì‚­ì œ ì™„ë£Œ: /home/clyde/dev_ws/deeplearning-repo-2/data/CCTV_Incident_Fall_Lying_Dataset/laying_dataset/labels/train.cache\n"
     ]
    }
   ],
   "source": [
    "# ìºì‹œ ì œê±° (ë ˆì´ë¸” ë³€ê²½ í›„ ìºì‹œê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš° ì˜¤ë¥˜ ë°©ì§€)\n",
    "import os\n",
    "\n",
    "cache_path = f'{base_dir}/labels/train.cache'\n",
    "if os.path.exists(cache_path):\n",
    "    os.remove(cache_path)\n",
    "    print('cache ì‚­ì œ ì™„ë£Œ:', cache_path)\n",
    "else:\n",
    "    print('cache ì—†ìŒ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3062d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== 1 epochë¡œ í•™ìŠµ ì‹œì‘ ====\n",
      "Ultralytics 8.3.240 ğŸš€ Python-3.12.3 torch-2.9.1+cu128 CPU (11th Gen Intel Core i5-1135G7 @ 2.40GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/clyde/dev_ws/deeplearning-repo-2/data/CCTV_Incident_Fall_Lying_Dataset/laying_dataset/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-pose.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=cctv_falllying_yolov8_pose_e1, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/clyde/dev_ws/deeplearning-repo-2/src/runs/train/cctv_falllying_yolov8_pose_e1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=pose, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=1 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1036129  ultralytics.nn.modules.head.Pose             [2, [17, 3], [64, 128, 256]]  \n",
      "YOLOv8n-pose summary: 144 layers, 3,295,665 parameters, 3,295,649 gradients, 9.3 GFLOPs\n",
      "\n",
      "Transferred 391/397 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1773.4Â±455.6 MB/s, size: 1462.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/clyde/dev_ws/deeplearning-repo-2/data/CCTV_Incident_Fall_Lying_Dataset/laying_dataset/labels/train... 88 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 88/88 295.1it/s 0.3s0.2s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/clyde/dev_ws/deeplearning-repo-2/data/CCTV_Incident_Fall_Lying_Dataset/laying_dataset/labels/train.cache\n",
      "WARNING âš ï¸ No 'flip_idx' array defined in data.yaml, disabling 'fliplr' and 'flipud' augmentations.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2741.7Â±175.3 MB/s, size: 1623.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/clyde/dev_ws/deeplearning-repo-2/data/CCTV_Incident_Fall_Lying_Dataset/laying_dataset/labels/val.cache... 23 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 23/23 20.5Kit/s 0.0s\n",
      "Plotting labels to /home/clyde/dev_ws/deeplearning-repo-2/src/runs/train/cctv_falllying_yolov8_pose_e1/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.0005), 72 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/home/clyde/dev_ws/deeplearning-repo-2/src/runs/train/cctv_falllying_yolov8_pose_e1\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/1         0G     0.8541      3.362     0.2922      2.194      1.197         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 22/22 2.8s/it 1:021.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.1s/it 3.3s2.0s\n",
      "                   all         23         24      0.456          1      0.742      0.529      0.395      0.891      0.575       0.26\n",
      "\n",
      "1 epochs completed in 0.019 hours.\n",
      "Optimizer stripped from /home/clyde/dev_ws/deeplearning-repo-2/src/runs/train/cctv_falllying_yolov8_pose_e1/weights/last.pt, 6.8MB\n",
      "Optimizer stripped from /home/clyde/dev_ws/deeplearning-repo-2/src/runs/train/cctv_falllying_yolov8_pose_e1/weights/best.pt, 6.8MB\n",
      "\n",
      "Validating /home/clyde/dev_ws/deeplearning-repo-2/src/runs/train/cctv_falllying_yolov8_pose_e1/weights/best.pt...\n",
      "Ultralytics 8.3.240 ğŸš€ Python-3.12.3 torch-2.9.1+cu128 CPU (11th Gen Intel Core i5-1135G7 @ 2.40GHz)\n",
      "YOLOv8n-pose summary (fused): 81 layers, 3,290,159 parameters, 0 gradients, 9.2 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 3/3 1.0it/s 2.9s2.0s\n",
      "                   all         23         24      0.459          1      0.742      0.529      0.397      0.891      0.575       0.26\n",
      "                  fall         23         23      0.523          1      0.987      0.879      0.409      0.783      0.652      0.345\n",
      "                 lying          1          1      0.394          1      0.497       0.18      0.386          1      0.497      0.175\n",
      "Speed: 1.0ms preprocess, 62.1ms inference, 0.0ms loss, 26.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/clyde/dev_ws/deeplearning-repo-2/src/runs/train/cctv_falllying_yolov8_pose_e1\u001b[0m\n",
      "==== 1 epoch í•™ìŠµ ì™„ë£Œ! ====\n"
     ]
    }
   ],
   "source": [
    "# Ultralytics YOLOv8 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° í•™ìŠµ (pose)\n",
    "from ultralytics import YOLO\n",
    "\n",
    "yaml_path = f'{base_dir}/data.yaml'\n",
    "model = YOLO('yolov8n-pose.pt')  # pose ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "# epochë¥¼ 1ë¡œ ì„¤ì •í•˜ì—¬ í•™ìŠµ (ì›í•˜ëŠ” epochë¡œ ì§ì ‘ ë³€ê²½ ê°€ëŠ¥)\n",
    "epochs = 1  # í•„ìš”ì‹œ ì‚¬ìš©ìê°€ ì§ì ‘ ë³€ê²½\n",
    "print(f'\\n==== {epochs} epochë¡œ í•™ìŠµ ì‹œì‘ ====')\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    epochs=epochs,\n",
    "    imgsz=640,\n",
    "    batch=4,\n",
    "    project='runs/train',\n",
    "    name=f'cctv_falllying_yolov8_pose_e{epochs}',\n",
    "    exist_ok=True,\n",
    "    device='cpu'  # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ 'cuda'\n",
    "    )\n",
    "print(f'==== {epochs} epoch í•™ìŠµ ì™„ë£Œ! ====')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194b09a6",
   "metadata": {},
   "source": [
    "## 5. ëª¨ë¸ í‰ê°€\n",
    "- í•™ìŠµì´ ëë‚œ í›„, ì£¼ìš” ì„±ëŠ¥ ì§€í‘œë¥¼ ìš”ì•½ ì¶œë ¥í•˜ê³ , ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc534d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ìµœì¢… Epoch: 1]\n",
      "metrics/precision(B): 0.4561\n",
      "metrics/recall(B): 1.0000\n",
      "metrics/mAP50(B): 0.7424\n",
      "metrics/mAP50-95(B): 0.5293\n",
      "metrics/precision(P): 0.3950\n",
      "metrics/recall(P): 0.8913\n",
      "metrics/mAP50(P): 0.5750\n",
      "metrics/mAP50-95(P): 0.2599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# í•™ìŠµ ê²°ê³¼ ìš”ì•½ ë° ì‹œê°í™” (pose)\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# training ê²°ê³¼ ì €ì¥ ê²½ë¡œ (results ê°ì²´ê°€ ìˆìœ¼ë©´ ì‚¬ìš©)\n",
    "run_dir = None\n",
    "if 'results' in globals() and hasattr(results, 'save_dir'):\n",
    "    run_dir = results.save_dir\n",
    "\n",
    "# fallback: ê°€ì¥ ìµœê·¼ run í´ë” ì°¾ê¸°\n",
    "if not run_dir:\n",
    "    run_base = os.path.join('runs', 'train')\n",
    "    if os.path.isdir(run_base):\n",
    "        runs = [os.path.join(run_base, d) for d in os.listdir(run_base)]\n",
    "        runs = [d for d in runs if os.path.isdir(d)]\n",
    "        if runs:\n",
    "            run_dir = max(runs, key=os.path.getmtime)\n",
    "\n",
    "if not run_dir:\n",
    "    raise FileNotFoundError('í•™ìŠµ ê²°ê³¼ í´ë”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.')\n",
    "\n",
    "result_csv = os.path.join(run_dir, 'results.csv')\n",
    "if os.path.exists(result_csv):\n",
    "    df = pd.read_csv(result_csv)\n",
    "    last = df.iloc[-1]\n",
    "    # pose + box ì§€í‘œ í•¨ê»˜ ì¶œë ¥ (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)\n",
    "    key_cols = [\n",
    "        'metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)',\n",
    "        'metrics/precision(P)', 'metrics/recall(P)', 'metrics/mAP50(P)', 'metrics/mAP50-95(P)'\n",
    "    ]\n",
    "    print(f\"[ìµœì¢… Epoch: {int(last['epoch'])}]\")\n",
    "    for k in key_cols:\n",
    "        if k in last:\n",
    "            print(f\"{k}: {last[k]:.4f}\")\n",
    "    # ê³¡ì„  ì‹œê°í™”\n",
    "    plot_cols = [c for c in key_cols if c in df.columns]\n",
    "    if plot_cols:\n",
    "        df[plot_cols].plot()\n",
    "        plt.title('YOLOv8 Pose í•™ìŠµ ì„±ëŠ¥ ì§€í‘œ ë³€í™”')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Score')\n",
    "        plt.show()\n",
    "else:\n",
    "    print('ê²°ê³¼ CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:', result_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2798dc",
   "metadata": {},
   "source": [
    "## 6. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì˜ˆì¸¡ ë° ê²°ê³¼ ì‹œê°í™”\n",
    "- í•™ìŠµëœ YOLO ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93f0a25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/clyde/dev_ws/deeplearning-repo-2/data/CCTV_Incident_Fall_Lying_Dataset/laying_dataset/images/val/laying65.png: 640x640 4 falls, 71.9ms\n",
      "Speed: 2.3ms preprocess, 71.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì˜ˆì¸¡ ë° ì‹œê°í™”\n",
    "from PIL import Image\n",
    "\n",
    "# ê²€ì¦ ì´ë¯¸ì§€ í´ë”\n",
    "test_img_dir = f'{base_dir}/images/val'\n",
    "img_files = [f for f in os.listdir(test_img_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "if not img_files:\n",
    "    raise FileNotFoundError(f'No images found in: {test_img_dir}')\n",
    "\n",
    "sample_img = os.path.join(test_img_dir, img_files[0])\n",
    "results = model.predict(sample_img, save=False, imgsz=640, conf=0.25)\n",
    "\n",
    "# ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”\n",
    "if hasattr(results[0], 'plot'):\n",
    "    results[0].plot()\n",
    "else:\n",
    "    print('ê²°ê³¼ ì‹œê°í™” ê¸°ëŠ¥ì€ ultralytics ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a2157",
   "metadata": {},
   "source": [
    "## 7. ëª¨ë¸ ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "- í•™ìŠµëœ YOLO ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ê³ , ì €ì¥ëœ ëª¨ë¸ì„ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì„ ì•ˆë‚´í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b270dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì €ì¥ëœ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì €ì¥ (YOLOv8ì€ ìë™ìœ¼ë¡œ best.pt, last.pt ì €ì¥)\n",
    "# ì¶”ê°€ë¡œ ì›í•˜ëŠ” ê²½ë¡œì— ì €ì¥í•˜ë ¤ë©´ ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©\n",
    "model_path = 'runs/train/cctv_falllying_yolov8_pose_e1/weights/best.pt'\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "loaded_model = YOLO(model_path)\n",
    "print('ì €ì¥ëœ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
